FamilySize 열 생성 완료!
train_dataset: 712, validation_dataset: 179, test_dataset: 418
Epoch 1/50, Training Loss: 0.7466926528347864, Validation Loss: 0.6476904029647509
Epoch 2/50, Training Loss: 0.6460284743044111, Validation Loss: 0.6430018991231918
Epoch 3/50, Training Loss: 0.6314773261547089, Validation Loss: 0.626421128710111
Epoch 4/50, Training Loss: 0.6183713310294681, Validation Loss: 0.613745724161466
Epoch 5/50, Training Loss: 0.5843577835294935, Validation Loss: 0.5913730015357336
Epoch 6/50, Training Loss: 0.56151688363817, Validation Loss: 0.5336224560936292
Epoch 7/50, Training Loss: 0.538569782839881, Validation Loss: 0.48761891573667526
Epoch 8/50, Training Loss: 0.5010935796631707, Validation Loss: 0.489689106742541
Epoch 9/50, Training Loss: 0.48957904908392164, Validation Loss: 0.4661873032649358
Epoch 10/50, Training Loss: 0.457579364379247, Validation Loss: 0.44101190691192943
Epoch 11/50, Training Loss: 0.4490892301003138, Validation Loss: 0.4528392205635707
Epoch 12/50, Training Loss: 0.4442621949646208, Validation Loss: 0.4627419967825214
Epoch 13/50, Training Loss: 0.4352083779043622, Validation Loss: 0.43732742592692375
Epoch 14/50, Training Loss: 0.4283204860157437, Validation Loss: 0.46293038378159207
Epoch 15/50, Training Loss: 0.4363645394643148, Validation Loss: 0.4696589969098568
Epoch 16/50, Training Loss: 0.42427369952201843, Validation Loss: 0.46010729918877286
Epoch 17/50, Training Loss: 0.4304749372932646, Validation Loss: 0.43794040754437447
Epoch 18/50, Training Loss: 0.4196194423569573, Validation Loss: 0.45187430332104367
Epoch 19/50, Training Loss: 0.42942524817254807, Validation Loss: 0.5493227864305178
Epoch 20/50, Training Loss: 0.4257361690203349, Validation Loss: 0.4827239935596784
Epoch 21/50, Training Loss: 0.4291897780365414, Validation Loss: 0.5156854589780172
Epoch 22/50, Training Loss: 0.4224459691180123, Validation Loss: 0.5212540626525879
Epoch 23/50, Training Loss: 0.4131592889626821, Validation Loss: 0.4483262784779072
Epoch 24/50, Training Loss: 0.41809030638800726, Validation Loss: 0.43195756897330284
Epoch 25/50, Training Loss: 0.41340012086762323, Validation Loss: 0.425739549100399
Epoch 26/50, Training Loss: 0.4162278456820382, Validation Loss: 0.47693726420402527
Epoch 27/50, Training Loss: 0.410464479525884, Validation Loss: 0.4252806808799505
Epoch 28/50, Training Loss: 0.4146765891048643, Validation Loss: 0.44381167987982434
Epoch 29/50, Training Loss: 0.4104239821434021, Validation Loss: 0.4729139382640521
Epoch 30/50, Training Loss: 0.41127643121613394, Validation Loss: 0.441936619579792
Epoch 31/50, Training Loss: 0.42545849780241646, Validation Loss: 0.47579339270790416
Epoch 32/50, Training Loss: 0.42261503438154857, Validation Loss: 0.5362051166594028
Epoch 33/50, Training Loss: 0.4158566696776284, Validation Loss: 0.44458900143702823
Epoch 34/50, Training Loss: 0.4128970583279928, Validation Loss: 0.45053871472676593
Epoch 35/50, Training Loss: 0.411293962597847, Validation Loss: 0.4756646404663722
Epoch 36/50, Training Loss: 0.40223503841294184, Validation Loss: 0.44590185955166817
Epoch 37/50, Training Loss: 0.4057104501459334, Validation Loss: 0.44716573009888333
Epoch 38/50, Training Loss: 0.40407828821076286, Validation Loss: 0.4555358290672302
Epoch 39/50, Training Loss: 0.4033474773168564, Validation Loss: 0.45606456448634464
Epoch 40/50, Training Loss: 0.42402689192030163, Validation Loss: 0.5127001019815604
Epoch 41/50, Training Loss: 0.4121747381157345, Validation Loss: 0.45336027070879936
Epoch 42/50, Training Loss: 0.405495066775216, Validation Loss: 0.5303366519510746
Epoch 43/50, Training Loss: 0.4068664242823919, Validation Loss: 0.4325644026199977
Epoch 44/50, Training Loss: 0.4052842057413525, Validation Loss: 0.4443461050589879
Epoch 45/50, Training Loss: 0.41252638366487293, Validation Loss: 0.42538443580269814
Epoch 46/50, Training Loss: 0.41029073463545906, Validation Loss: 0.42045629024505615
Epoch 47/50, Training Loss: 0.4034645878606372, Validation Loss: 0.43057512616117793
Epoch 48/50, Training Loss: 0.4054901189274258, Validation Loss: 0.4455752484500408
Epoch 49/50, Training Loss: 0.4018613818618986, Validation Loss: 0.41944249781469506
Epoch 50/50, Training Loss: 0.40220542848110197, Validation Loss: 0.44486267616351444
[TEST DATASET]
input shape: torch.Size([418, 6])
892 0
893 1
894 0
895 0
896 1
897 0
898 1
899 0
900 1
901 0
902 0
903 0
904 1
905 0
906 1
907 1
908 0
909 0
910 1
911 1
912 0
913 0
914 1
915 1
916 1
917 0
918 1
919 0
920 0
921 0
922 0
923 0
924 1
925 1
926 1
927 0
928 1
929 1
930 0
931 0
932 0
933 0
934 0
935 1
936 1
937 0
938 0
939 0
940 1
941 1
942 0
943 0
944 1
945 1
946 0
947 0
948 0
949 0
950 0
951 1
952 0
953 0
954 0
955 1
956 0
957 1
958 1
959 0
960 0
961 1
962 1
963 0
964 1
965 0
966 1
967 1
968 0
969 1
970 0
971 1
972 1
973 0
974 0
975 0
976 0
977 0
978 1
979 1
980 1
981 1
982 1
983 0
984 1
985 0
986 0
987 0
988 1
989 0
990 1
991 0
992 1
993 0
994 0
995 0
996 1
997 0
998 0
999 0
1000 0
1001 0
1002 0
1003 1
1004 1
1005 1
1006 1
1007 0
1008 0
1009 1
1010 1
1011 1
1012 1
1013 0
1014 1
1015 0
1016 0
1017 1
1018 0
1019 1
1020 0
1021 0
1022 0
1023 0
1024 0
1025 0
1026 0
1027 0
1028 0
1029 0
1030 1
1031 0
1032 0
1033 1
1034 0
1035 0
1036 0
1037 0
1038 0
1039 0
1040 0
1041 0
1042 1
1043 0
1044 0
1045 1
1046 0
1047 0
1048 1
1049 1
1050 0
1051 1
1052 1
1053 0
1054 1
1055 0
1056 0
1057 1
1058 0
1059 0
1060 1
1061 1
1062 0
1063 0
1064 0
1065 0
1066 0
1067 1
1068 1
1069 0
1070 1
1071 1
1072 0
1073 0
1074 1
1075 0
1076 1
1077 0
1078 1
1079 0
1080 0
1081 0
1082 0
1083 0
1084 0
1085 0
1086 0
1087 0
1088 1
1089 1
1090 0
1091 1
1092 1
1093 1
1094 1
1095 1
1096 0
1097 0
1098 1
1099 0
1100 1
1101 0
1102 0
1103 0
1104 0
1105 1
1106 0
1107 0
1108 1
1109 0
1110 1
1111 0
1112 1
1113 0
1114 1
1115 0
1116 1
1117 1
1118 0
1119 1
1120 0
1121 0
1122 0
1123 1
1124 0
1125 0
1126 0
1127 0
1128 0
1129 0
1130 1
1131 1
1132 1
1133 1
1134 1
1135 0
1136 0
1137 0
1138 1
1139 0
1140 1
1141 1
1142 0
1143 0
1144 1
1145 0
1146 0
1147 0
1148 0
1149 0
1150 1
1151 0
1152 0
1153 0
1154 1
1155 1
1156 0
1157 0
1158 0
1159 0
1160 1
1161 0
1162 0
1163 0
1164 1
1165 1
1166 0
1167 1
1168 0
1169 0
1170 0
1171 0
1172 1
1173 1
1174 1
1175 1
1176 1
1177 0
1178 0
1179 0
1180 0
1181 0
1182 0
1183 1
1184 0
1185 0
1186 0
1187 0
1188 1
1189 0
1190 0
1191 0
1192 0
1193 0
1194 0
1195 0
1196 1
1197 1
1198 0
1199 1
1200 0
1201 1
1202 0
1203 0
1204 0
1205 1
1206 1
1207 1
1208 0
1209 0
1210 0
1211 0
1212 0
1213 0
1214 0
1215 0
1216 1
1217 0
1218 1
1219 1
1220 0
1221 0
1222 1
1223 0
1224 0
1225 1
1226 0
1227 0
1228 0
1229 0
1230 0
1231 0
1232 0
1233 0
1234 0
1235 1
1236 0
1237 1
1238 0
1239 1
1240 0
1241 1
1242 1
1243 0
1244 0
1245 0
1246 1
1247 0
1248 1
1249 0
1250 0
1251 1
1252 0
1253 1
1254 1
1255 0
1256 1
1257 0
1258 0
1259 1
1260 1
1261 0
1262 0
1263 1
1264 0
1265 0
1266 1
1267 1
1268 1
1269 0
1270 0
1271 0
1272 0
1273 0
1274 1
1275 1
1276 0
1277 1
1278 0
1279 0
1280 0
1281 0
1282 1
1283 1
1284 0
1285 0
1286 0
1287 1
1288 0
1289 1
1290 0
1291 0
1292 1
1293 0
1294 1
1295 0
1296 0
1297 0
1298 0
1299 1
1300 1
1301 1
1302 1
1303 1
1304 1
1305 0
1306 1
1307 0
1308 0
1309 0
