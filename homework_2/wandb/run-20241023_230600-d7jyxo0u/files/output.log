FamilySize 열 생성 완료!
train_dataset: 712, validation_dataset: 179, test_dataset: 418
Epoch 1/50, Training Loss: 0.6853033165136974, Validation Loss: 0.6616408377885818
Epoch 2/50, Training Loss: 0.640741413169437, Validation Loss: 0.6145109211405119
Epoch 3/50, Training Loss: 0.6096688707669576, Validation Loss: 0.6015511850516001
Epoch 4/50, Training Loss: 0.5743379712104797, Validation Loss: 0.5625103761752447
Epoch 5/50, Training Loss: 0.5401314205593533, Validation Loss: 0.5459605505069097
Epoch 6/50, Training Loss: 0.5117376936806572, Validation Loss: 0.5241379862030348
Epoch 7/50, Training Loss: 0.49106564389334784, Validation Loss: 0.522627721230189
Epoch 8/50, Training Loss: 0.45682229631476934, Validation Loss: 0.530637642989556
Epoch 9/50, Training Loss: 0.44175309505727556, Validation Loss: 0.5114738121628761
Epoch 10/50, Training Loss: 0.42673975858423446, Validation Loss: 0.5250773752729098
Epoch 11/50, Training Loss: 0.4279711845848295, Validation Loss: 0.5126305893063545
Epoch 12/50, Training Loss: 0.4326158089770211, Validation Loss: 0.5248229838907719
Epoch 13/50, Training Loss: 0.42043010592460633, Validation Loss: 0.501010850071907
Epoch 14/50, Training Loss: 0.4106720762120353, Validation Loss: 0.48270804435014725
Epoch 15/50, Training Loss: 0.40987480696704653, Validation Loss: 0.5755117312073708
Epoch 16/50, Training Loss: 0.4146835916572147, Validation Loss: 0.5680178155501684
Epoch 17/50, Training Loss: 0.43313820593886904, Validation Loss: 0.5040109592179457
Epoch 18/50, Training Loss: 0.4095039735237757, Validation Loss: 0.47137905688335496
Epoch 19/50, Training Loss: 0.4081330269575119, Validation Loss: 0.5341990167895952
Epoch 20/50, Training Loss: 0.41142375502321454, Validation Loss: 0.5395031720399857
Epoch 21/50, Training Loss: 0.41181133025222355, Validation Loss: 0.47049872390925884
Epoch 22/50, Training Loss: 0.4031929721434911, Validation Loss: 0.49650730689366657
Epoch 23/50, Training Loss: 0.4148475444979138, Validation Loss: 0.47984667122364044
Epoch 24/50, Training Loss: 0.41779744095272486, Validation Loss: 0.4703952732185523
Epoch 25/50, Training Loss: 0.3986793627341588, Validation Loss: 0.4830121075113614
Epoch 26/50, Training Loss: 0.4081833004951477, Validation Loss: 0.4665279584005475
Epoch 27/50, Training Loss: 0.39904623760117425, Validation Loss: 0.5003239884972572
Epoch 28/50, Training Loss: 0.40470655924744076, Validation Loss: 0.4860422263542811
Epoch 29/50, Training Loss: 0.4044566184282303, Validation Loss: 0.487203411757946
Epoch 30/50, Training Loss: 0.39568884869416554, Validation Loss: 0.4702512143800656
Epoch 31/50, Training Loss: 0.4022895872592926, Validation Loss: 0.4677037826428811
Epoch 32/50, Training Loss: 0.4032382594214545, Validation Loss: 0.4661468354364236
Epoch 33/50, Training Loss: 0.3998969402578142, Validation Loss: 0.5370660449067751
Epoch 34/50, Training Loss: 0.4107753818233808, Validation Loss: 0.5097744291027387
Epoch 35/50, Training Loss: 0.3937610685825348, Validation Loss: 0.4794834976394971
Epoch 36/50, Training Loss: 0.3990760422415204, Validation Loss: 0.6597872041165829
Epoch 37/50, Training Loss: 0.420060161418385, Validation Loss: 0.5318967178463936
Epoch 38/50, Training Loss: 0.40781615045335556, Validation Loss: 0.529966164380312
Epoch 39/50, Training Loss: 0.3966870973507563, Validation Loss: 0.46903742104768753
Epoch 40/50, Training Loss: 0.40322069459491305, Validation Loss: 0.5040721471110979
Epoch 41/50, Training Loss: 0.3969059593147702, Validation Loss: 0.5182600344220797
Epoch 42/50, Training Loss: 0.3988254838519626, Validation Loss: 0.4753237081070741
Epoch 43/50, Training Loss: 0.39330713583363425, Validation Loss: 0.4656908717006445
Epoch 44/50, Training Loss: 0.3976983196205563, Validation Loss: 0.47325415164232254
Epoch 45/50, Training Loss: 0.3949560718403922, Validation Loss: 0.49471328407526016
Epoch 46/50, Training Loss: 0.3976350122027927, Validation Loss: 0.46800925210118294
Epoch 47/50, Training Loss: 0.3898957825369305, Validation Loss: 0.45966277954479057
Epoch 48/50, Training Loss: 0.38744522598054676, Validation Loss: 0.5385940000414848
Epoch 49/50, Training Loss: 0.39362938735220165, Validation Loss: 0.4880516976118088
Epoch 50/50, Training Loss: 0.3899141076538298, Validation Loss: 0.46907089091837406
[TEST DATASET]
input shape: torch.Size([418, 6])
892 0
893 0
894 0
895 0
896 0
897 0
898 1
899 0
900 1
901 0
902 0
903 0
904 1
905 0
906 1
907 1
908 0
909 0
910 0
911 1
912 0
913 0
914 1
915 0
916 1
917 0
918 1
919 0
920 0
921 0
922 0
923 0
924 0
925 0
926 0
927 0
928 1
929 1
930 0
931 0
932 0
933 0
934 0
935 1
936 1
937 0
938 0
939 0
940 1
941 0
942 0
943 0
944 1
945 1
946 0
947 0
948 0
949 0
950 0
951 1
952 0
953 0
954 0
955 1
956 0
957 1
958 1
959 0
960 0
961 1
962 1
963 0
964 1
965 0
966 1
967 1
968 0
969 1
970 0
971 1
972 0
973 0
974 0
975 0
976 0
977 0
978 1
979 1
980 1
981 1
982 1
983 0
984 1
985 0
986 1
987 0
988 1
989 0
990 1
991 0
992 1
993 0
994 0
995 0
996 1
997 0
998 0
999 0
1000 0
1001 0
1002 0
1003 1
1004 1
1005 1
1006 1
1007 0
1008 0
1009 1
1010 1
1011 1
1012 1
1013 0
1014 1
1015 0
1016 0
1017 1
1018 0
1019 1
1020 0
1021 0
1022 0
1023 0
1024 0
1025 0
1026 0
1027 0
1028 0
1029 0
1030 1
1031 0
1032 0
1033 1
1034 0
1035 0
1036 0
1037 0
1038 0
1039 0
1040 0
1041 0
1042 1
1043 0
1044 0
1045 0
1046 0
1047 0
1048 1
1049 1
1050 0
1051 0
1052 1
1053 0
1054 1
1055 0
1056 0
1057 1
1058 0
1059 0
1060 1
1061 1
1062 0
1063 0
1064 0
1065 0
1066 0
1067 1
1068 1
1069 0
1070 1
1071 1
1072 0
1073 0
1074 1
1075 0
1076 1
1077 0
1078 1
1079 0
1080 0
1081 0
1082 0
1083 0
1084 0
1085 0
1086 1
1087 0
1088 1
1089 1
1090 0
1091 1
1092 1
1093 1
1094 1
1095 1
1096 0
1097 0
1098 1
1099 0
1100 1
1101 0
1102 0
1103 0
1104 0
1105 1
1106 0
1107 0
1108 1
1109 0
1110 1
1111 0
1112 1
1113 0
1114 1
1115 0
1116 1
1117 1
1118 0
1119 1
1120 0
1121 0
1122 1
1123 1
1124 0
1125 0
1126 0
1127 0
1128 0
1129 0
1130 1
1131 1
1132 1
1133 1
1134 0
1135 0
1136 0
1137 0
1138 1
1139 0
1140 1
1141 1
1142 1
1143 0
1144 1
1145 0
1146 0
1147 0
1148 0
1149 0
1150 1
1151 0
1152 0
1153 0
1154 1
1155 1
1156 0
1157 0
1158 0
1159 0
1160 1
1161 0
1162 0
1163 0
1164 1
1165 1
1166 0
1167 1
1168 0
1169 0
1170 0
1171 0
1172 1
1173 1
1174 1
1175 1
1176 1
1177 0
1178 0
1179 0
1180 0
1181 0
1182 0
1183 1
1184 0
1185 0
1186 0
1187 0
1188 1
1189 0
1190 0
1191 0
1192 0
1193 0
1194 0
1195 0
1196 1
1197 1
1198 0
1199 1
1200 0
1201 1
1202 0
1203 0
1204 0
1205 1
1206 1
1207 1
1208 0
1209 0
1210 0
1211 0
1212 0
1213 0
1214 0
1215 0
1216 1
1217 0
1218 1
1219 0
1220 0
1221 0
1222 1
1223 0
1224 0
1225 1
1226 0
1227 0
1228 0
1229 0
1230 0
1231 0
1232 0
1233 0
1234 0
1235 1
1236 0
1237 1
1238 0
1239 1
1240 0
1241 1
1242 1
1243 0
1244 0
1245 0
1246 1
1247 0
1248 1
1249 0
1250 0
1251 1
1252 0
1253 1
1254 1
1255 0
1256 1
1257 0
1258 0
1259 1
1260 1
1261 0
1262 0
1263 1
1264 0
1265 0
1266 1
1267 1
1268 0
1269 0
1270 0
1271 0
1272 0
1273 0
1274 1
1275 1
1276 0
1277 1
1278 0
1279 0
1280 0
1281 0
1282 1
1283 1
1284 0
1285 0
1286 0
1287 1
1288 0
1289 1
1290 0
1291 0
1292 1
1293 0
1294 1
1295 0
1296 0
1297 0
1298 0
1299 0
1300 1
1301 1
1302 1
1303 1
1304 1
1305 0
1306 1
1307 0
1308 0
1309 0
