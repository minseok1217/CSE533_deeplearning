{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요구사항 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/2_data_server/cv-07/link_dl/_02_homeworks/homework_2/wandb/run-20241023_230346-ujg1lo5j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch/runs/ujg1lo5j' target=\"_blank\">desert-eon-11</a></strong> to <a href='https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch' target=\"_blank\">https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch/runs/ujg1lo5j' target=\"_blank\">https://wandb.ai/ping5432121-korea-university-of-technology-and-education/titanic-pytorch/runs/ujg1lo5j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB 프로젝트 초기화\n",
    "# wandb.init을 사용하여 \"titanic-pytorch\"라는 이름으로 Weights & Biases(WANDB) 프로젝트를 초기화함.\n",
    "# 이를 통해 실험 결과를 추적하고 시각화할 수 있음.\n",
    "wandb.init(project=\"titanic-pytorch\")\n",
    "\n",
    "# TitanicDataset 정의\n",
    "# PyTorch의 Dataset 클래스를 상속하여 TitanicDataset을 정의함.\n",
    "# X는 입력 데이터(특징), y는 레이블(목표 값)임.\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 입력 데이터 X를 FloatTensor로 변환하여 self.X에 저장.\n",
    "        # y(레이블)를 LongTensor로 변환하여 self.y에 저장.\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환하는 메서드.\n",
    "        # len(self.X)를 호출하여 데이터의 총 개수를 반환함.\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터를 가져옴.\n",
    "        # feature는 self.X에서 입력 데이터를, target은 self.y에서 레이블을 가져옴.\n",
    "        # 두 값을 딕셔너리 형태로 반환하여 모델 학습에 사용될 수 있도록 함.\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return {'input': feature, 'target': target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicTestDataset 정의\n",
    "# PyTorch의 Dataset 클래스를 상속하여 TitanicTestDataset을 정의함.\n",
    "# 테스트 데이터셋으로, 레이블(y)이 없고 입력 데이터(X)만 처리함.\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # 입력 데이터 X를 FloatTensor로 변환하여 self.X에 저장.\n",
    "        self.X = torch.FloatTensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환하는 메서드.\n",
    "        # len(self.X)를 호출하여 데이터의 총 개수를 반환함.\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터를 가져옴.\n",
    "        # feature는 self.X에서 입력 데이터를 가져와 반환함.\n",
    "        # 테스트 데이터이므로 타겟(레이블)이 없으며, 'input'만 딕셔너리 형태로 반환함.\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보(크기 및 입력 데이터의 모양)를 문자열로 반환하는 메서드.\n",
    "        # len(self.X)는 데이터셋 크기, self.X.shape는 입력 데이터의 형태를 나타냄.\n",
    "        return \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수들 정의\n",
    "\n",
    "# 결측된 나이(Age) 값을 평균 나이로 채움.\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    all_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n",
    "    return all_df\n",
    "\n",
    "# 성별(Sex)을 숫자로 변환. 남성은 0, 여성은 1로 맵핑.\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    all_df['Sex'] = all_df['Sex'].map({'male': 0, 'female': 1})\n",
    "    return all_df\n",
    "\n",
    "# 'Cabin', 'Ticket', 'Name' 열 삭제. 모델 학습에 불필요한 데이터를 제거.\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    all_df = all_df.drop(columns=['Cabin', 'Ticket', 'Name'])\n",
    "    return all_df\n",
    "\n",
    "# 탑승 항구(Embarked) 결측값을 최빈값으로 채우고, 'C', 'Q', 'S'를 각각 0, 1, 2로 맵핑.\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df['Embarked'] = all_df['Embarked'].fillna(all_df['Embarked'].mode()[0])\n",
    "    all_df['Embarked'] = all_df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "    return all_df\n",
    "\n",
    "# 운임(Fare)에 로그 변환을 적용하여 데이터의 분포를 정규화함.\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    all_df['Fare'] = np.log1p(all_df['Fare'])\n",
    "    return all_df\n",
    "\n",
    "# 형제 자매(SibSp)와 부모 자식(Parch)을 합하여 가족 크기(FamilySize) 열을 생성.\n",
    "# 만약 'SibSp' 또는 'Parch' 열이 없으면 KeyError를 발생시킴.\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    if 'SibSp' in all_df.columns and 'Parch' in all_df.columns:\n",
    "        all_df['FamilySize'] = all_df['SibSp'] + all_df['Parch']  # FamilySize 생성\n",
    "        print(\"FamilySize 열 생성 완료!\")  # 확인용 출력\n",
    "    else:\n",
    "        raise KeyError(\"'SibSp' 또는 'Parch' 열이 없습니다.\")\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 전처리하는 함수 정의\n",
    "# Titanic 데이터셋을 전처리하고, 학습, 검증, 테스트 데이터셋을 반환함.\n",
    "\n",
    "def get_preprocessed_dataset():\n",
    "    # 현재 작업 중인 디렉토리 경로를 가져옴.\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "    # train.csv와 test.csv 파일 경로를 생성함.\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    # train.csv와 test.csv 파일을 읽어 각각 데이터프레임으로 저장함.\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    # train_df와 test_df를 하나의 데이터프레임으로 결합함. Survived 열을 기준으로 결합됨.\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 각각의 전처리 함수들을 호출하여 데이터프레임을 전처리함.\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # Survived 값이 존재하는 행은 train_df로, 존재하지 않는 행은 test_df로 나눔.\n",
    "    train_df = all_df.loc[all_df['Survived'].notnull()]\n",
    "    test_df = all_df.loc[all_df['Survived'].isnull()]\n",
    "\n",
    "    # 사용할 특징(feature)들과 라벨(label)을 설정함.\n",
    "    features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize']\n",
    "    label = 'Survived'\n",
    "\n",
    "    # 학습 데이터셋(X_train)과 라벨(y_train)을 배열로 변환.\n",
    "    X_train = train_df[features].values\n",
    "    y_train = train_df[label].values\n",
    "\n",
    "    # 테스트 데이터셋(X_test)을 배열로 변환.\n",
    "    X_test = test_df[features].values\n",
    "\n",
    "    # TitanicDataset을 사용해 전체 학습 데이터셋과 테스트 데이터셋을 생성함.\n",
    "    full_train_dataset = TitanicDataset(X_train, y_train)\n",
    "    test_dataset = TitanicTestDataset(X_test)\n",
    "\n",
    "    # 학습 데이터셋의 80%는 훈련(train) 데이터로, 20%는 검증(validation) 데이터로 나눔.\n",
    "    train_size = int(0.8 * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "    # random_split을 사용하여 학습 데이터셋과 검증 데이터셋을 나눔.\n",
    "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    # 전처리된 학습, 검증, 테스트 데이터셋을 반환함.\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# PyTorch의 nn.Module을 상속받아 MyModel 클래스를 정의함.\n",
    "# 이 모델은 간단한 완전 연결 신경망(fully connected neural network)으로 구성됨.\n",
    "\n",
    "class MyModel_relu(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        # 부모 클래스(nn.Module)의 초기화 메서드를 호출하여 초기화.\n",
    "        super(MyModel_relu, self).__init__()\n",
    "        \n",
    "        # nn.Sequential을 사용해 모델의 레이어들을 차례대로 정의.\n",
    "        # 입력층: n_input -> 30 유닛 (Linear 레이어)\n",
    "        # 활성화 함수로 ReLU를 사용함.\n",
    "        # 은닉층: 30 -> 30 유닛 (Linear 레이어) + ReLU 활성화 함수\n",
    "        # 출력층: 30 -> n_output 유닛 (Linear 레이어)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),  # 입력층: n_input -> 30\n",
    "            nn.ReLU(),  # 활성화 함수 ReLU\n",
    "            nn.Linear(30, 30),  # 은닉층: 30 -> 30\n",
    "            nn.ReLU(),  # 활성화 함수 ReLU\n",
    "            nn.Linear(30, n_output)  # 출력층: 30 -> n_output\n",
    "        )\n",
    "\n",
    "    # forward 메서드는 순전파(입력에서 출력으로) 과정을 정의.\n",
    "    # 입력 데이터 x를 모델에 전달하고 결과값을 반환.\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# PyTorch의 nn.Module을 상속받아 MyModel 클래스를 정의함.\n",
    "# 이 모델은 간단한 완전 연결 신경망(fully connected neural network)으로 구성됨.\n",
    "\n",
    "class MyModel_sigmoid(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        # 부모 클래스(nn.Module)의 초기화 메서드를 호출하여 초기화.\n",
    "        super(MyModel_sigmoid, self).__init__()\n",
    "        \n",
    "        # nn.Sequential을 사용해 모델의 레이어들을 차례대로 정의.\n",
    "        # 입력층: n_input -> 30 유닛 (Linear 레이어)\n",
    "        # 활성화 함수로 Sigmoid를 사용함.\n",
    "        # 은닉층: 30 -> 30 유닛 (Linear 레이어) + Sigmoid 활성화 함수\n",
    "        # 출력층: 30 -> n_output 유닛 (Linear 레이어)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),  # 입력층: n_input -> 30\n",
    "            nn.Sigmoid(),  # 활성화 함수 Sigmoid\n",
    "            nn.Linear(30, 30),  # 은닉층: 30 -> 30\n",
    "            nn.Sigmoid(),  # 활성화 함수 Sigmoid\n",
    "            nn.Linear(30, n_output)  # 출력층: 30 -> n_output\n",
    "        )\n",
    "\n",
    "    # forward 메서드는 순전파(입력에서 출력으로) 과정을 정의.\n",
    "    # 입력 데이터 x를 모델에 전달하고 결과값을 반환.\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 정의\n",
    "# 주어진 모델을 학습시키고 검증하는 함수 train_model을 정의함.\n",
    "# 이 함수는 학습 손실과 검증 손실을 계산하고, WANDB에 로그를 기록함.\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, epochs=10):\n",
    "    # 손실 함수로 CrossEntropyLoss 사용 (분류 문제에서 주로 사용됨).\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MultiMarginLoss()\n",
    "    \n",
    "    # Adam 옵티마이저를 사용하여 학습률 0.001로 설정.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 에포크(epochs) 동안 학습을 반복함.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 모델을 학습 모드로 전환.\n",
    "        train_loss = 0  # 한 에포크 동안의 훈련 손실을 저장할 변수.\n",
    "\n",
    "        # 훈련 데이터로 학습.\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['input']  # 입력 데이터를 가져옴.\n",
    "            targets = batch['target']  # 타겟(레이블) 데이터를 가져옴.\n",
    "\n",
    "            optimizer.zero_grad()  # 옵티마이저의 그라디언트 초기화.\n",
    "            outputs = model(inputs)  # 모델을 통해 예측값을 얻음.\n",
    "            loss = criterion(outputs, targets)  # 손실을 계산.\n",
    "            loss.backward()  # 손실에 대한 그라디언트를 계산 (역전파).\n",
    "            optimizer.step()  # 옵티마이저를 통해 모델 파라미터를 업데이트.\n",
    "\n",
    "            train_loss += loss.item()  # 배치 손실을 총 학습 손실에 더함.\n",
    "\n",
    "        model.eval()  # 모델을 평가 모드로 전환.\n",
    "        validation_loss = 0  # 한 에포크 동안의 검증 손실을 저장할 변수.\n",
    "        \n",
    "        # 검증 데이터로 모델 평가. 평가 중에는 그라디언트 계산을 하지 않음.\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                inputs = batch['input']  # 입력 데이터를 가져옴.\n",
    "                targets = batch['target']  # 타겟(레이블) 데이터를 가져옴.\n",
    "\n",
    "                outputs = model(inputs)  # 모델을 통해 예측값을 얻음.\n",
    "                loss = criterion(outputs, targets)  # 손실을 계산.\n",
    "                validation_loss += loss.item()  # 배치 손실을 총 검증 손실에 더함.\n",
    "\n",
    "        # WANDB에 학습 및 검증 손실을 로그로 기록.\n",
    "        # wandb.log({\n",
    "        #     'train_loss': train_loss / len(train_loader),  # 평균 학습 손실\n",
    "        #     'validation_loss': validation_loss / len(validation_loader),  # 평균 검증 손실\n",
    "        #     'epoch': epoch + 1  # 현재 에포크\n",
    "        # })\n",
    "\n",
    "        # 현재 에포크에 대한 학습 및 검증 손실을 출력.\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {validation_loss / len(validation_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요구사항 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FamilySize 열 생성 완료!\n",
      "train_dataset: 712, validation_dataset: 179, test_dataset: 418\n",
      "Epoch 1/50, Training Loss: 0.4153092846274376, Validation Loss: 0.3287919784585635\n",
      "Epoch 2/50, Training Loss: 0.400051778058211, Validation Loss: 0.3464181733628114\n",
      "Epoch 3/50, Training Loss: 0.39741280211342705, Validation Loss: 0.3229856143395106\n",
      "Epoch 4/50, Training Loss: 0.394527311457528, Validation Loss: 0.31749054541190463\n",
      "Epoch 5/50, Training Loss: 0.39060713946819303, Validation Loss: 0.2967113384753854\n",
      "Epoch 6/50, Training Loss: 0.3857061243719525, Validation Loss: 0.31878162175416946\n",
      "Epoch 7/50, Training Loss: 0.3815948837333255, Validation Loss: 0.2948843302826087\n",
      "Epoch 8/50, Training Loss: 0.3795913795630137, Validation Loss: 0.3096452976266543\n",
      "Epoch 9/50, Training Loss: 0.3663637323511971, Validation Loss: 0.30856217443943024\n",
      "Epoch 10/50, Training Loss: 0.3538209019435777, Validation Loss: 0.3035861986378829\n",
      "Epoch 11/50, Training Loss: 0.34378210968441436, Validation Loss: 0.32646247868736583\n",
      "Epoch 12/50, Training Loss: 0.3194610993067423, Validation Loss: 0.2582089633991321\n",
      "Epoch 13/50, Training Loss: 0.2925249526898066, Validation Loss: 0.23516520578414202\n",
      "Epoch 14/50, Training Loss: 0.27366991639137267, Validation Loss: 0.24139472097158432\n",
      "Epoch 15/50, Training Loss: 0.26081213487519156, Validation Loss: 0.21980128375192484\n",
      "Epoch 16/50, Training Loss: 0.2574428447418743, Validation Loss: 0.21928894643982252\n",
      "Epoch 17/50, Training Loss: 0.25835931930277084, Validation Loss: 0.2167997875561317\n",
      "Epoch 18/50, Training Loss: 0.2475611608889368, Validation Loss: 0.2194074218471845\n",
      "Epoch 19/50, Training Loss: 0.24701199928919473, Validation Loss: 0.2242669245849053\n",
      "Epoch 20/50, Training Loss: 0.2463756420546108, Validation Loss: 0.21807224676012993\n",
      "Epoch 21/50, Training Loss: 0.24345991495582792, Validation Loss: 0.21118638788660368\n",
      "Epoch 22/50, Training Loss: 0.2388342837492625, Validation Loss: 0.19994917387763658\n",
      "Epoch 23/50, Training Loss: 0.24108821087413365, Validation Loss: 0.20862087234854698\n",
      "Epoch 24/50, Training Loss: 0.23544900930590099, Validation Loss: 0.20108008260528246\n",
      "Epoch 25/50, Training Loss: 0.23326649251911374, Validation Loss: 0.20228172279894352\n",
      "Epoch 26/50, Training Loss: 0.23657046622700162, Validation Loss: 0.1760372631251812\n",
      "Epoch 27/50, Training Loss: 0.2318238452076912, Validation Loss: 0.1835897999505202\n",
      "Epoch 28/50, Training Loss: 0.23026198198397954, Validation Loss: 0.19709266163408756\n",
      "Epoch 29/50, Training Loss: 0.23138467719157538, Validation Loss: 0.20773458542923132\n",
      "Epoch 30/50, Training Loss: 0.23228710658020443, Validation Loss: 0.17865928324560323\n",
      "Epoch 31/50, Training Loss: 0.22678937514623007, Validation Loss: 0.19199835198620954\n",
      "Epoch 32/50, Training Loss: 0.22684021873606575, Validation Loss: 0.21740596989790598\n",
      "Epoch 33/50, Training Loss: 0.2236386465529601, Validation Loss: 0.17899429549773535\n",
      "Epoch 34/50, Training Loss: 0.22581306708355744, Validation Loss: 0.19906356806556383\n",
      "Epoch 35/50, Training Loss: 0.2264911264181137, Validation Loss: 0.1752990591727818\n",
      "Epoch 36/50, Training Loss: 0.22753767205609216, Validation Loss: 0.19927853594223657\n",
      "Epoch 37/50, Training Loss: 0.22313081754578484, Validation Loss: 0.16608382357905307\n",
      "Epoch 38/50, Training Loss: 0.22297504047552744, Validation Loss: 0.18221389905860028\n",
      "Epoch 39/50, Training Loss: 0.22383863363001083, Validation Loss: 0.17635979565481344\n",
      "Epoch 40/50, Training Loss: 0.21869919300079346, Validation Loss: 0.18296410702168941\n",
      "Epoch 41/50, Training Loss: 0.2202671632170677, Validation Loss: 0.16598431393504143\n",
      "Epoch 42/50, Training Loss: 0.21808768742614323, Validation Loss: 0.18112602395315966\n",
      "Epoch 43/50, Training Loss: 0.21680057363377678, Validation Loss: 0.18861668991545835\n",
      "Epoch 44/50, Training Loss: 0.21819548913174205, Validation Loss: 0.18407574637482563\n",
      "Epoch 45/50, Training Loss: 0.21876096874475479, Validation Loss: 0.16105899835626283\n",
      "Epoch 46/50, Training Loss: 0.21749066445562576, Validation Loss: 0.17864362709224224\n",
      "Epoch 47/50, Training Loss: 0.21556101044019063, Validation Loss: 0.1632998405645291\n",
      "Epoch 48/50, Training Loss: 0.21722857769992615, Validation Loss: 0.1793179187613229\n",
      "Epoch 49/50, Training Loss: 0.21512660566303465, Validation Loss: 0.16132710315287113\n",
      "Epoch 50/50, Training Loss: 0.21489522949688963, Validation Loss: 0.16615222052981457\n",
      "[TEST DATASET]\n",
      "input shape: torch.Size([418, 6])\n",
      "예측 결과가 submission_2.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # WANDB 초기화\n",
    "    # Weights & Biases(WANDB)에서 실험 추적을 위해 프로젝트 \"titanic-pytorch\"를 초기화.\n",
    "    # wandb.init(project=\"titanic-pytorch\")\n",
    "    # wandb.init(project=\"titanic-pytorch\", mode=\"offline\")\n",
    "\n",
    "\n",
    "    # 전처리된 데이터셋을 로드\n",
    "    # get_preprocessed_dataset 함수를 호출하여 학습, 검증, 테스트 데이터셋을 가져옴.\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "    # 각 데이터셋의 크기를 출력\n",
    "    print(\"train_dataset: {0}, validation_dataset: {1}, test_dataset: {2}\".format(\n",
    "        len(train_dataset), len(validation_dataset), len(test_dataset)))\n",
    "\n",
    "    # DataLoader 생성\n",
    "    # train_dataset과 validation_dataset을 각각 배치 크기 16으로 로드. 훈련 데이터를 섞어서 셔플링.\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # test_dataset은 전체를 한 번에 로드하기 위해 batch_size를 데이터셋 크기로 설정.\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 정의 및 학습\n",
    "    # 입력 차원은 6 (Pclass, Sex, Age, Fare, Embarked, FamilySize), 출력 차원은 2 (생존 여부).\n",
    "    my_model = MyModel_sigmoid(n_input=6, n_output=2)\n",
    "\n",
    "    # 모델을 학습시키고, 50번의 에포크 동안 train_loader와 validation_loader를 통해 학습.\n",
    "    train_model(my_model, train_data_loader, validation_data_loader, epochs=50)\n",
    "\n",
    "    # Test 데이터셋에 대해 예측 생성\n",
    "    print(\"[TEST DATASET]\")\n",
    "    test_sample = next(iter(test_data_loader))\n",
    "    print(\"input shape: {0}\".format(test_sample['input'].shape))\n",
    "\n",
    "    output_batch = my_model(test_sample['input'])\n",
    "    prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "\n",
    "    # ender_submission.csv 파일을 불러옴\n",
    "    submission = pd.read_csv('gender_submission.csv')\n",
    "\n",
    "    # 예측한 값을 'Survived' 열에 추가\n",
    "    submission['Survived'] = prediction_batch.cpu().numpy()\n",
    "\n",
    "    # 결과를 새로운 CSV 파일로 저장\n",
    "    submission.to_csv('submission_sigmoid_svm.csv', index=False)\n",
    "\n",
    "    print(\"예측 결과가 submission_2.csv에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요구사항 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rank.png\" alt=\"Image Description\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./submission.png\" alt=\"Image Description\" width=\"300\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
